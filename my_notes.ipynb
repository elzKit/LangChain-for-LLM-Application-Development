{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d07fcc-ded7-4bd0-8419-7c8d1957a850",
   "metadata": {},
   "source": [
    "# My notes from the course\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c694c218-2b7b-4ae4-ba3f-03db56b3d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4a77455-270e-48f8-8f22-bbbb19f0e98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  /home/elzbieta/genai/LangChain-for-LLM-Application-Development/.venv/bin/python -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  /home/elzbieta/genai/LangChain-for-LLM-Application-Development/.venv/bin/python -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  /home/elzbieta/genai/LangChain-for-LLM-Application-Development/.venv/bin/python -m pip install [options] [-e] <vcs project url> ...\n",
      "  /home/elzbieta/genai/LangChain-for-LLM-Application-Development/.venv/bin/python -m pip install [options] [-e] <local project path> ...\n",
      "  /home/elzbieta/genai/LangChain-for-LLM-Application-Development/.venv/bin/python -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -u\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-openai -quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736786ba-9c5c-4994-abe6-bd423d845982",
   "metadata": {},
   "source": [
    "# Langchain has reusable prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e587c3a-9a0d-43e7-a129-c58a7d9afd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a2e4bbb-f06c-415f-bad5-3550a418f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=\"excited French\",\n",
    "                    text=\"Hello, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c85daeac-544f-4117-b257-dc313a3d1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d9e7ae4-2921-4ffa-aa5a-176c8fd13e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages_translated = llm.invoke(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd6a028f-2397-4581-85fe-2234f8dd1a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Bonjour, je m'appelle Andrew!\" response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 36, 'total_tokens': 44}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c0783fb-0941-425a-8f79-6bc1ee6bf30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=\"very formal German\",\n",
    "                    text=\"Hello, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ef38632-20cb-48ce-8a8a-2c554ffd2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=llm.invoke(customer_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da57832-f683-4923-a919-aacce17a399d",
   "metadata": {},
   "source": [
    "it looks like I can invoke prompts but also directly strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "199d2215-7d2a-468c-bacd-f10de33501b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, I do not know your name as I am an AI assistant and do not have access to personal information.\", response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 11, 'total_tokens': 36}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is my name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b537b-8919-41b9-846d-68019536d191",
   "metadata": {},
   "source": [
    "# Prompts can have output parsers\n",
    "\n",
    "See L1 notebook, output parsers, for how to extract directly a JSON from the LLM output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a04b67-85a7-4b09-9ec5-a9e3da174bf6",
   "metadata": {},
   "source": [
    "# Simple ConversationChain\n",
    "By default, the ConversationChain has a simple type of memory that remembers all previous inputs/outputs and adds them to the context that is passed to the LLM (see ConversationBufferMemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c072c171-5c77-4473-906f-9084134fe222",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2ada5f6-c137-4e6f-b362-d9c99374e214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ade6adb-a854-46f1-aa53-54cb28a4377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is my name\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Andrew.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f86656-85df-429c-b25d-34a62d4b9e00",
   "metadata": {},
   "source": [
    "# Does openAI now have memory by default? Somehow with the key?\n",
    "\n",
    "The answer is :NO.\n",
    "To prove it, I want to do 2 accesses and check if it remembers the name from the 1.st access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a2d4a4-5f07-4af3-94d8-cecfa1df1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "client.chat.completions.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5f6a203-8c03-42d7-966e-879d3c7c3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca5254c7-9ee1-4df0-9a97-f8609c08059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b24aa8e7-1a97-4e7b-8b8b-6b70767b7328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Andrew! How can I assist you today?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"My name is andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21f9282a-fe09-4df4-a38c-7ab424548a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not know your name as I am an AI assistant and do not have access to personal information.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e77386-4133-4ae5-8918-c5ec2c755ff9",
   "metadata": {},
   "source": [
    "So, if I make to API requests, there is no memory between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe94fc86-0e62-4b76-8220-f236b4d7ad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-96h62ziDJuFVwLC2O4IVLWNEU6pig', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm an AI and I don't have teeth, so I can't experience tooth pain. Is there anything else you would like to know or discuss?\", role='assistant', function_call=None, tool_calls=None))], created=1711382658, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3bc1b5746c', usage=CompletionUsage(completion_tokens=31, prompt_tokens=20, total_tokens=51))\n"
     ]
    }
   ],
   "source": [
    "# Test sending multiple prompts to openAI\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"tell me about Warsaw uprising\"}, {\"role\": \"user\", \"content\": \"Does your tooth hurt\"}, ]\n",
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d945ac-e997-49e9-baf7-527374a3077b",
   "metadata": {},
   "source": [
    "My little experiment did not work, only the second message got processed. TODO: understand the usage of messages being a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1989b5-f61b-4cb6-8bd9-677dfc784f18",
   "metadata": {},
   "source": [
    "# ConverstationChain with explicit memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdaeb33-6858-4d43-a871-fe3037025484",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation2 = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    "    memory = memory\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
